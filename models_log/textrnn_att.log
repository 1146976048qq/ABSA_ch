
0it [00:00, ?it/s]
2114it [00:00, 21134.18it/s]
4891it [00:00, 22765.93it/s]
7584it [00:00, 23867.95it/s]
9286it [00:00, 20986.11it/s]
11174it [00:00, 20304.87it/s]
13499it [00:00, 21106.81it/s]
15900it [00:00, 21900.81it/s]
18404it [00:00, 22756.67it/s]
21187it [00:00, 24072.91it/s]
23818it [00:01, 24702.45it/s]
26470it [00:01, 25220.54it/s]
29118it [00:01, 25582.49it/s]
31861it [00:01, 26106.47it/s]
34604it [00:01, 26488.76it/s]
36850it [00:01, 24402.27it/s]

0it [00:00, ?it/s]
1655it [00:00, 11939.82it/s]
3068it [00:00, 12521.83it/s]
4940it [00:00, 15610.56it/s]

0it [00:00, ?it/s]
2766it [00:00, 27649.08it/s]
4940it [00:00, 27904.90it/s]
Loading data...
Vocab size: 4762
Time usage: 0:00:02
<bound method Module.parameters of Model(
  (embedding): Embedding(4762, 300)
  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)
  (tanh1): Tanh()
  (tanh2): Tanh()
  (fc1): Linear(in_features=256, out_features=64, bias=True)
  (fc): Linear(in_features=64, out_features=3, bias=True)
)>
Epoch [1/10]
Iter:      0,  Train Loss:   1.1,  Train Acc: 10.16%,  Val Loss:   1.0,  Val Acc: 77.78%,  Time: 0:00:00 *
Iter:    100,  Train Loss:  0.62,  Train Acc: 78.12%,  Val Loss:  0.63,  Val Acc: 77.88%,  Time: 0:00:02 *
Iter:    200,  Train Loss:  0.67,  Train Acc: 76.56%,  Val Loss:  0.62,  Val Acc: 78.58%,  Time: 0:00:03 *
Epoch [2/10]
Iter:    300,  Train Loss:  0.69,  Train Acc: 74.22%,  Val Loss:   0.6,  Val Acc: 78.76%,  Time: 0:00:05 *
Iter:    400,  Train Loss:  0.52,  Train Acc: 83.59%,  Val Loss:   0.6,  Val Acc: 78.91%,  Time: 0:00:07 *
Iter:    500,  Train Loss:  0.62,  Train Acc: 78.91%,  Val Loss:  0.61,  Val Acc: 79.01%,  Time: 0:00:08 
Epoch [3/10]
Iter:    600,  Train Loss:  0.59,  Train Acc: 79.69%,  Val Loss:  0.62,  Val Acc: 78.66%,  Time: 0:00:10 
Iter:    700,  Train Loss:  0.47,  Train Acc: 86.72%,  Val Loss:   0.6,  Val Acc: 79.07%,  Time: 0:00:11 *
Iter:    800,  Train Loss:  0.51,  Train Acc: 83.59%,  Val Loss:   0.6,  Val Acc: 79.01%,  Time: 0:00:13 
Epoch [4/10]
Iter:    900,  Train Loss:  0.41,  Train Acc: 86.72%,  Val Loss:   0.6,  Val Acc: 79.01%,  Time: 0:00:14 
Iter:   1000,  Train Loss:  0.63,  Train Acc: 73.44%,  Val Loss:   0.6,  Val Acc: 78.87%,  Time: 0:00:16 
Iter:   1100,  Train Loss:  0.49,  Train Acc: 84.38%,  Val Loss:  0.62,  Val Acc: 78.27%,  Time: 0:00:18 
Epoch [5/10]
Iter:   1200,  Train Loss:  0.64,  Train Acc: 77.34%,  Val Loss:   0.6,  Val Acc: 79.03%,  Time: 0:00:19 *
Iter:   1300,  Train Loss:  0.43,  Train Acc: 85.94%,  Val Loss:  0.61,  Val Acc: 78.82%,  Time: 0:00:21 
Iter:   1400,  Train Loss:   0.6,  Train Acc: 76.56%,  Val Loss:  0.62,  Val Acc: 78.39%,  Time: 0:00:22 
Epoch [6/10]
Iter:   1500,  Train Loss:  0.59,  Train Acc: 78.12%,  Val Loss:  0.61,  Val Acc: 78.72%,  Time: 0:00:24 
Iter:   1600,  Train Loss:  0.51,  Train Acc: 81.25%,  Val Loss:  0.73,  Val Acc: 75.53%,  Time: 0:00:25 
Iter:   1700,  Train Loss:  0.61,  Train Acc: 75.78%,  Val Loss:  0.67,  Val Acc: 77.30%,  Time: 0:00:27 
Epoch [7/10]
Iter:   1800,  Train Loss:  0.42,  Train Acc: 85.94%,  Val Loss:  0.65,  Val Acc: 78.62%,  Time: 0:00:29 
Iter:   1900,  Train Loss:   0.3,  Train Acc: 90.62%,  Val Loss:  0.72,  Val Acc: 75.62%,  Time: 0:00:30 
Iter:   2000,  Train Loss:  0.44,  Train Acc: 81.25%,  Val Loss:  0.72,  Val Acc: 74.61%,  Time: 0:00:32 
Epoch [8/10]
Iter:   2100,  Train Loss:  0.52,  Train Acc: 82.81%,  Val Loss:  0.67,  Val Acc: 77.22%,  Time: 0:00:33 
Iter:   2200,  Train Loss:  0.42,  Train Acc: 84.38%,  Val Loss:   0.7,  Val Acc: 78.66%,  Time: 0:00:35 
No optimization for a long time, auto-stopping...
Test Loss:  0.59,  Test Acc: 79.98%

Time usage: 0:00:00
