
0it [00:00, ?it/s]
2187it [00:00, 21864.96it/s]
3587it [00:00, 18709.74it/s]
6176it [00:00, 20406.71it/s]
8777it [00:00, 21814.14it/s]
11106it [00:00, 22236.25it/s]
13033it [00:00, 19329.90it/s]
15326it [00:00, 20284.40it/s]
17784it [00:00, 21405.36it/s]
20320it [00:00, 22455.54it/s]
22759it [00:01, 23001.53it/s]
25221it [00:01, 23462.46it/s]
27742it [00:01, 23959.54it/s]
30439it [00:01, 24788.37it/s]
32926it [00:01, 21827.24it/s]
35179it [00:01, 21057.41it/s]
36850it [00:01, 22303.51it/s]

0it [00:00, ?it/s]
1655it [00:00, 13002.71it/s]
4621it [00:00, 15636.60it/s]
4940it [00:00, 20646.65it/s]

0it [00:00, ?it/s]
2850it [00:00, 28490.92it/s]
4940it [00:00, 28355.72it/s]
Loading data...
Vocab size: 4762
Time usage: 0:00:02
<bound method Module.parameters of Model(
  (embedding): Embedding(4762, 300)
  (conv_region): Conv2d(1, 250, kernel_size=(3, 300), stride=(1, 1))
  (conv): Conv2d(250, 250, kernel_size=(3, 1), stride=(1, 1))
  (max_pool): MaxPool2d(kernel_size=(3, 1), stride=2, padding=0, dilation=1, ceil_mode=False)
  (padding1): ZeroPad2d(padding=(0, 0, 1, 1), value=0.0)
  (padding2): ZeroPad2d(padding=(0, 0, 0, 1), value=0.0)
  (relu): ReLU()
  (fc): Linear(in_features=250, out_features=3, bias=True)
)>
Epoch [1/20]
Iter:      0,  Train Loss:   1.1,  Train Acc: 18.75%,  Val Loss:  0.74,  Val Acc: 77.78%,  Time: 0:00:01 *
Iter:    100,  Train Loss:  0.67,  Train Acc: 78.12%,  Val Loss:  0.65,  Val Acc: 77.78%,  Time: 0:00:03 *
Iter:    200,  Train Loss:  0.68,  Train Acc: 76.56%,  Val Loss:  0.63,  Val Acc: 78.64%,  Time: 0:00:05 *
Epoch [2/20]
Iter:    300,  Train Loss:  0.69,  Train Acc: 75.00%,  Val Loss:  0.61,  Val Acc: 78.50%,  Time: 0:00:06 *
Iter:    400,  Train Loss:   0.5,  Train Acc: 85.16%,  Val Loss:  0.61,  Val Acc: 78.84%,  Time: 0:00:08 *
Iter:    500,  Train Loss:  0.62,  Train Acc: 78.12%,  Val Loss:  0.61,  Val Acc: 78.99%,  Time: 0:00:10 
Epoch [3/20]
Iter:    600,  Train Loss:  0.59,  Train Acc: 78.91%,  Val Loss:  0.61,  Val Acc: 78.84%,  Time: 0:00:12 
Iter:    700,  Train Loss:  0.46,  Train Acc: 88.28%,  Val Loss:   0.6,  Val Acc: 79.07%,  Time: 0:00:14 *
Iter:    800,  Train Loss:   0.5,  Train Acc: 82.81%,  Val Loss:   0.6,  Val Acc: 79.21%,  Time: 0:00:16 
Epoch [4/20]
Iter:    900,  Train Loss:  0.38,  Train Acc: 88.28%,  Val Loss:  0.61,  Val Acc: 79.05%,  Time: 0:00:18 
Iter:   1000,  Train Loss:  0.61,  Train Acc: 74.22%,  Val Loss:  0.64,  Val Acc: 77.75%,  Time: 0:00:20 
Iter:   1100,  Train Loss:  0.52,  Train Acc: 81.25%,  Val Loss:  0.63,  Val Acc: 78.66%,  Time: 0:00:22 
Epoch [5/20]
Iter:   1200,  Train Loss:   0.6,  Train Acc: 75.78%,  Val Loss:  0.62,  Val Acc: 78.74%,  Time: 0:00:23 
Iter:   1300,  Train Loss:  0.43,  Train Acc: 87.50%,  Val Loss:  0.65,  Val Acc: 77.78%,  Time: 0:00:25 
Iter:   1400,  Train Loss:  0.54,  Train Acc: 79.69%,  Val Loss:  0.65,  Val Acc: 78.21%,  Time: 0:00:27 
Epoch [6/20]
Iter:   1500,  Train Loss:  0.48,  Train Acc: 80.47%,  Val Loss:  0.71,  Val Acc: 76.30%,  Time: 0:00:29 
Iter:   1600,  Train Loss:  0.45,  Train Acc: 82.03%,  Val Loss:   0.7,  Val Acc: 76.07%,  Time: 0:00:31 
Iter:   1700,  Train Loss:  0.59,  Train Acc: 75.78%,  Val Loss:  0.74,  Val Acc: 77.57%,  Time: 0:00:32 
No optimization for a long time, auto-stopping...
Test Loss:  0.59,  Test Acc: 79.69%

Time usage: 0:00:00
