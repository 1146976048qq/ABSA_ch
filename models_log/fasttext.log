
0it [00:00, ?it/s]
1305it [00:00, 13043.95it/s]
2145it [00:00, 11187.42it/s]
3288it [00:00, 11258.23it/s]
4397it [00:00, 11206.86it/s]
5424it [00:00, 10907.84it/s]
6657it [00:00, 11297.96it/s]
7650it [00:00, 9430.92it/s] 
8587it [00:00, 9412.33it/s]
9498it [00:00, 9301.52it/s]
10498it [00:01, 9499.08it/s]
11651it [00:01, 10028.88it/s]
12653it [00:01, 8874.51it/s] 
13803it [00:01, 9526.28it/s]
15064it [00:01, 10278.61it/s]
16179it [00:01, 10522.79it/s]
17328it [00:01, 10795.26it/s]
18524it [00:01, 11119.74it/s]
19654it [00:02, 7596.55it/s] 
20576it [00:02, 7835.85it/s]
21523it [00:02, 8261.92it/s]
22437it [00:02, 8315.72it/s]
23330it [00:02, 7898.95it/s]
24588it [00:02, 8891.41it/s]
25856it [00:02, 9765.96it/s]
27041it [00:02, 10309.35it/s]
28214it [00:02, 10696.55it/s]
29486it [00:02, 11231.43it/s]
30650it [00:03, 11260.02it/s]
31864it [00:03, 11508.16it/s]
33058it [00:03, 11634.00it/s]
34237it [00:03, 11382.35it/s]
35388it [00:03, 10812.95it/s]
36609it [00:03, 11194.91it/s]
36850it [00:03, 10112.36it/s]

0it [00:00, ?it/s]
1080it [00:00, 10799.21it/s]
2170it [00:00, 10826.38it/s]
3394it [00:00, 11214.12it/s]
4612it [00:00, 11486.95it/s]
4940it [00:00, 11593.63it/s]

0it [00:00, ?it/s]
1043it [00:00, 3972.02it/s]
2303it [00:00, 4998.72it/s]
3304it [00:00, 5881.33it/s]
4008it [00:00, 6185.94it/s]
4940it [00:00, 7502.61it/s]
Loading data...
Vocab size: 4762
Time usage: 0:00:05
<bound method Module.parameters of Model(
  (embedding): Embedding(4762, 300, padding_idx=4761)
  (embedding_ngram2): Embedding(250499, 300)
  (embedding_ngram3): Embedding(250499, 300)
  (dropout): Dropout(p=0.5)
  (fc1): Linear(in_features=900, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=3, bias=True)
)>
Epoch [1/20]
Iter:      0,  Train Loss:   1.3,  Train Acc: 14.84%,  Val Loss:  0.99,  Val Acc: 62.11%,  Time: 0:00:08 *
Iter:    100,  Train Loss:  0.66,  Train Acc: 78.12%,  Val Loss:  0.64,  Val Acc: 77.86%,  Time: 0:00:19 *
Iter:    200,  Train Loss:  0.66,  Train Acc: 75.78%,  Val Loss:  0.64,  Val Acc: 78.15%,  Time: 0:00:30 *
Epoch [2/20]
Iter:    300,  Train Loss:  0.69,  Train Acc: 73.44%,  Val Loss:  0.65,  Val Acc: 78.35%,  Time: 0:00:34 
Iter:    400,  Train Loss:  0.56,  Train Acc: 82.81%,  Val Loss:  0.62,  Val Acc: 78.35%,  Time: 0:00:46 *
Iter:    500,  Train Loss:   0.6,  Train Acc: 78.12%,  Val Loss:  0.61,  Val Acc: 78.47%,  Time: 0:00:57 *
Epoch [3/20]
Iter:    600,  Train Loss:  0.59,  Train Acc: 78.12%,  Val Loss:  0.61,  Val Acc: 78.62%,  Time: 0:01:09 *
Iter:    700,  Train Loss:  0.48,  Train Acc: 85.16%,  Val Loss:  0.61,  Val Acc: 78.60%,  Time: 0:01:22 *
Iter:    800,  Train Loss:   0.5,  Train Acc: 83.59%,  Val Loss:  0.61,  Val Acc: 78.56%,  Time: 0:01:27 
Epoch [4/20]
Iter:    900,  Train Loss:  0.35,  Train Acc: 88.28%,  Val Loss:  0.61,  Val Acc: 78.62%,  Time: 0:01:33 
Iter:   1000,  Train Loss:  0.56,  Train Acc: 74.22%,  Val Loss:  0.62,  Val Acc: 78.33%,  Time: 0:01:39 
Iter:   1100,  Train Loss:  0.45,  Train Acc: 83.59%,  Val Loss:  0.62,  Val Acc: 78.66%,  Time: 0:01:44 
Epoch [5/20]
Iter:   1200,  Train Loss:  0.52,  Train Acc: 79.69%,  Val Loss:  0.63,  Val Acc: 78.52%,  Time: 0:01:50 
Iter:   1300,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.65,  Val Acc: 78.29%,  Time: 0:01:55 
Iter:   1400,  Train Loss:  0.49,  Train Acc: 82.81%,  Val Loss:  0.65,  Val Acc: 77.96%,  Time: 0:02:01 
Epoch [6/20]
Iter:   1500,  Train Loss:  0.48,  Train Acc: 81.25%,  Val Loss:  0.67,  Val Acc: 77.18%,  Time: 0:02:06 
Iter:   1600,  Train Loss:  0.42,  Train Acc: 83.59%,  Val Loss:  0.68,  Val Acc: 77.94%,  Time: 0:02:12 
Iter:   1700,  Train Loss:  0.45,  Train Acc: 82.03%,  Val Loss:  0.69,  Val Acc: 77.80%,  Time: 0:02:18 
No optimization for a long time, auto-stopping...
Test Loss:  0.55,  Test Acc: 80.61%

Time usage: 0:00:00
