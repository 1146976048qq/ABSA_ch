
0it [00:00, ?it/s]
2628it [00:00, 26272.63it/s]
5161it [00:00, 25980.77it/s]
7747it [00:00, 25944.41it/s]
10334it [00:00, 25921.81it/s]
12854it [00:00, 25699.92it/s]
15509it [00:00, 25941.58it/s]
18120it [00:00, 25991.40it/s]
20675it [00:00, 25856.07it/s]
23110it [00:00, 20452.16it/s]
25675it [00:01, 21775.62it/s]
28265it [00:01, 22866.33it/s]
30618it [00:01, 20291.91it/s]
33240it [00:01, 21768.24it/s]
35517it [00:01, 19838.01it/s]
36850it [00:01, 22694.56it/s]

0it [00:00, ?it/s]
1646it [00:00, 14152.75it/s]
4120it [00:00, 16236.80it/s]
4940it [00:00, 19853.91it/s]

0it [00:00, ?it/s]
1847it [00:00, 18469.31it/s]
3568it [00:00, 18069.14it/s]
4940it [00:00, 19228.75it/s]
Loading data...
Vocab size: 4762
Time usage: 0:00:02
<bound method Module.parameters of Model(
  (embedding): Embedding(4762, 300)
  (postion_embedding): Positional_Encoding(
    (dropout): Dropout(p=0.5)
  )
  (encoder): Encoder(
    (attention): Multi_Head_Attention(
      (fc_Q): Linear(in_features=300, out_features=300, bias=True)
      (fc_K): Linear(in_features=300, out_features=300, bias=True)
      (fc_V): Linear(in_features=300, out_features=300, bias=True)
      (attention): Scaled_Dot_Product_Attention()
      (fc): Linear(in_features=300, out_features=300, bias=True)
      (dropout): Dropout(p=0.5)
      (layer_norm): LayerNorm(torch.Size([300]), eps=1e-05, elementwise_affine=True)
    )
    (feed_forward): Position_wise_Feed_Forward(
      (fc1): Linear(in_features=300, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=300, bias=True)
      (dropout): Dropout(p=0.5)
      (layer_norm): LayerNorm(torch.Size([300]), eps=1e-05, elementwise_affine=True)
    )
  )
  (encoders): ModuleList(
    (0): Encoder(
      (attention): Multi_Head_Attention(
        (fc_Q): Linear(in_features=300, out_features=300, bias=True)
        (fc_K): Linear(in_features=300, out_features=300, bias=True)
        (fc_V): Linear(in_features=300, out_features=300, bias=True)
        (attention): Scaled_Dot_Product_Attention()
        (fc): Linear(in_features=300, out_features=300, bias=True)
        (dropout): Dropout(p=0.5)
        (layer_norm): LayerNorm(torch.Size([300]), eps=1e-05, elementwise_affine=True)
      )
      (feed_forward): Position_wise_Feed_Forward(
        (fc1): Linear(in_features=300, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=300, bias=True)
        (dropout): Dropout(p=0.5)
        (layer_norm): LayerNorm(torch.Size([300]), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Encoder(
      (attention): Multi_Head_Attention(
        (fc_Q): Linear(in_features=300, out_features=300, bias=True)
        (fc_K): Linear(in_features=300, out_features=300, bias=True)
        (fc_V): Linear(in_features=300, out_features=300, bias=True)
        (attention): Scaled_Dot_Product_Attention()
        (fc): Linear(in_features=300, out_features=300, bias=True)
        (dropout): Dropout(p=0.5)
        (layer_norm): LayerNorm(torch.Size([300]), eps=1e-05, elementwise_affine=True)
      )
      (feed_forward): Position_wise_Feed_Forward(
        (fc1): Linear(in_features=300, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=300, bias=True)
        (dropout): Dropout(p=0.5)
        (layer_norm): LayerNorm(torch.Size([300]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (fc1): Linear(in_features=9600, out_features=3, bias=True)
)>
Epoch [1/20]
Iter:      0,  Train Loss:   1.2,  Train Acc: 28.91%,  Val Loss:   2.7,  Val Acc: 77.78%,  Time: 0:00:01 *
Iter:    100,  Train Loss:  0.76,  Train Acc: 78.12%,  Val Loss:  0.77,  Val Acc: 77.78%,  Time: 0:00:04 *
Iter:    200,  Train Loss:  0.71,  Train Acc: 76.56%,  Val Loss:  0.69,  Val Acc: 77.78%,  Time: 0:00:07 *
Epoch [2/20]
Iter:    300,  Train Loss:  0.78,  Train Acc: 74.22%,  Val Loss:  0.67,  Val Acc: 77.78%,  Time: 0:00:12 *
Iter:    400,  Train Loss:  0.66,  Train Acc: 82.03%,  Val Loss:  0.72,  Val Acc: 77.78%,  Time: 0:00:15 
Iter:    500,  Train Loss:   0.7,  Train Acc: 78.12%,  Val Loss:  0.77,  Val Acc: 77.78%,  Time: 0:00:17 
Epoch [3/20]
Iter:    600,  Train Loss:   0.7,  Train Acc: 77.34%,  Val Loss:  0.76,  Val Acc: 77.78%,  Time: 0:00:19 
Iter:    700,  Train Loss:   0.6,  Train Acc: 84.38%,  Val Loss:  0.67,  Val Acc: 77.78%,  Time: 0:00:25 *
Iter:    800,  Train Loss:  0.63,  Train Acc: 81.25%,  Val Loss:  0.69,  Val Acc: 77.78%,  Time: 0:00:28 
Epoch [4/20]
Iter:    900,  Train Loss:  0.47,  Train Acc: 85.94%,  Val Loss:  0.85,  Val Acc: 77.78%,  Time: 0:00:30 
Iter:   1000,  Train Loss:  0.74,  Train Acc: 71.09%,  Val Loss:  0.65,  Val Acc: 77.86%,  Time: 0:00:32 *
Iter:   1100,  Train Loss:  0.65,  Train Acc: 78.91%,  Val Loss:  0.67,  Val Acc: 78.21%,  Time: 0:00:40 
Epoch [5/20]
Iter:   1200,  Train Loss:  0.71,  Train Acc: 75.00%,  Val Loss:  0.63,  Val Acc: 78.17%,  Time: 0:00:48 *
Iter:   1300,  Train Loss:  0.53,  Train Acc: 85.16%,  Val Loss:  0.68,  Val Acc: 77.98%,  Time: 0:00:55 
Iter:   1400,  Train Loss:  0.82,  Train Acc: 73.44%,  Val Loss:  0.72,  Val Acc: 77.98%,  Time: 0:01:03 
Epoch [6/20]
Iter:   1500,  Train Loss:   0.7,  Train Acc: 76.56%,  Val Loss:  0.66,  Val Acc: 78.64%,  Time: 0:01:11 
Iter:   1600,  Train Loss:  0.61,  Train Acc: 82.03%,  Val Loss:   0.7,  Val Acc: 78.43%,  Time: 0:01:19 
Iter:   1700,  Train Loss:  0.77,  Train Acc: 75.00%,  Val Loss:  0.67,  Val Acc: 78.33%,  Time: 0:01:27 
Epoch [7/20]
Iter:   1800,  Train Loss:  0.47,  Train Acc: 84.38%,  Val Loss:  0.63,  Val Acc: 78.84%,  Time: 0:01:31 *
Iter:   1900,  Train Loss:  0.52,  Train Acc: 84.38%,  Val Loss:  0.68,  Val Acc: 78.54%,  Time: 0:01:33 
Iter:   2000,  Train Loss:   0.6,  Train Acc: 76.56%,  Val Loss:  0.61,  Val Acc: 78.72%,  Time: 0:01:35 *
Epoch [8/20]
Iter:   2100,  Train Loss:  0.66,  Train Acc: 78.12%,  Val Loss:  0.61,  Val Acc: 78.72%,  Time: 0:01:37 
Iter:   2200,  Train Loss:  0.57,  Train Acc: 82.81%,  Val Loss:  0.67,  Val Acc: 78.82%,  Time: 0:01:39 
Iter:   2300,  Train Loss:  0.54,  Train Acc: 85.16%,  Val Loss:  0.63,  Val Acc: 78.89%,  Time: 0:01:41 
Epoch [9/20]
Iter:   2400,  Train Loss:   0.6,  Train Acc: 79.69%,  Val Loss:  0.62,  Val Acc: 78.78%,  Time: 0:01:44 
Iter:   2500,  Train Loss:  0.62,  Train Acc: 78.91%,  Val Loss:  0.62,  Val Acc: 78.80%,  Time: 0:01:46 
Epoch [10/20]
Iter:   2600,  Train Loss:  0.64,  Train Acc: 75.78%,  Val Loss:  0.62,  Val Acc: 79.03%,  Time: 0:01:48 
Iter:   2700,  Train Loss:  0.57,  Train Acc: 82.03%,  Val Loss:  0.62,  Val Acc: 78.82%,  Time: 0:01:50 
Iter:   2800,  Train Loss:   0.6,  Train Acc: 78.12%,  Val Loss:  0.61,  Val Acc: 78.64%,  Time: 0:01:52 
Epoch [11/20]
Iter:   2900,  Train Loss:  0.61,  Train Acc: 75.78%,  Val Loss:  0.61,  Val Acc: 78.93%,  Time: 0:01:54 
Iter:   3000,  Train Loss:  0.56,  Train Acc: 80.47%,  Val Loss:  0.63,  Val Acc: 79.05%,  Time: 0:01:56 
Iter:   3100,  Train Loss:  0.58,  Train Acc: 80.47%,  Val Loss:  0.62,  Val Acc: 79.05%,  Time: 0:01:58 
Epoch [12/20]
Iter:   3200,  Train Loss:  0.61,  Train Acc: 77.34%,  Val Loss:  0.63,  Val Acc: 78.84%,  Time: 0:02:00 
Iter:   3300,  Train Loss:  0.49,  Train Acc: 85.94%,  Val Loss:  0.62,  Val Acc: 79.17%,  Time: 0:02:02 
Iter:   3400,  Train Loss:  0.78,  Train Acc: 67.97%,  Val Loss:  0.61,  Val Acc: 79.13%,  Time: 0:02:05 *
Epoch [13/20]
Iter:   3500,  Train Loss:  0.55,  Train Acc: 79.69%,  Val Loss:  0.64,  Val Acc: 78.84%,  Time: 0:02:07 
Iter:   3600,  Train Loss:  0.73,  Train Acc: 75.78%,  Val Loss:  0.65,  Val Acc: 79.13%,  Time: 0:02:09 
Iter:   3700,  Train Loss:  0.47,  Train Acc: 82.81%,  Val Loss:  0.64,  Val Acc: 77.30%,  Time: 0:02:11 
Epoch [14/20]
Iter:   3800,  Train Loss:  0.58,  Train Acc: 78.91%,  Val Loss:  0.62,  Val Acc: 79.15%,  Time: 0:02:13 
Iter:   3900,  Train Loss:  0.53,  Train Acc: 80.47%,  Val Loss:  0.63,  Val Acc: 78.76%,  Time: 0:02:15 
Iter:   4000,  Train Loss:  0.54,  Train Acc: 81.25%,  Val Loss:  0.66,  Val Acc: 79.03%,  Time: 0:02:17 
Epoch [15/20]
Iter:   4100,  Train Loss:  0.58,  Train Acc: 78.91%,  Val Loss:  0.77,  Val Acc: 78.68%,  Time: 0:02:19 
Iter:   4200,  Train Loss:  0.56,  Train Acc: 77.34%,  Val Loss:  0.61,  Val Acc: 78.97%,  Time: 0:02:22 
Iter:   4300,  Train Loss:  0.55,  Train Acc: 81.25%,  Val Loss:  0.62,  Val Acc: 78.95%,  Time: 0:02:24 
Epoch [16/20]
Iter:   4400,  Train Loss:  0.55,  Train Acc: 78.91%,  Val Loss:  0.65,  Val Acc: 79.03%,  Time: 0:02:26 
Iter:   4500,  Train Loss:  0.56,  Train Acc: 79.69%,  Val Loss:  0.61,  Val Acc: 78.84%,  Time: 0:02:28 
Iter:   4600,  Train Loss:  0.49,  Train Acc: 82.03%,  Val Loss:  0.67,  Val Acc: 78.93%,  Time: 0:02:30 
Epoch [17/20]
Iter:   4700,  Train Loss:  0.63,  Train Acc: 75.78%,  Val Loss:  0.66,  Val Acc: 78.89%,  Time: 0:02:32 
Iter:   4800,  Train Loss:  0.46,  Train Acc: 82.03%,  Val Loss:  0.62,  Val Acc: 78.95%,  Time: 0:02:34 
Epoch [18/20]
Iter:   4900,  Train Loss:  0.61,  Train Acc: 77.34%,  Val Loss:  0.63,  Val Acc: 77.94%,  Time: 0:02:36 
Iter:   5000,  Train Loss:   0.6,  Train Acc: 78.91%,  Val Loss:  0.71,  Val Acc: 78.97%,  Time: 0:02:38 
Iter:   5100,  Train Loss:  0.49,  Train Acc: 83.59%,  Val Loss:  0.66,  Val Acc: 78.84%,  Time: 0:02:40 
Epoch [19/20]
Iter:   5200,  Train Loss:  0.63,  Train Acc: 78.12%,  Val Loss:  0.63,  Val Acc: 79.11%,  Time: 0:02:42 
Iter:   5300,  Train Loss:  0.53,  Train Acc: 80.47%,  Val Loss:   0.7,  Val Acc: 78.99%,  Time: 0:02:44 
Iter:   5400,  Train Loss:  0.45,  Train Acc: 83.59%,  Val Loss:  0.66,  Val Acc: 78.00%,  Time: 0:02:46 
No optimization for a long time, auto-stopping...
Test Loss:  0.56,  Test Acc: 80.83%

Time usage: 0:00:00
