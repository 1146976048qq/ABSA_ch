
0it [00:00, ?it/s]
1896it [00:00, 18957.22it/s]
4586it [00:00, 20798.67it/s]
7379it [00:00, 22521.80it/s]
10056it [00:00, 23647.18it/s]
12860it [00:00, 24813.01it/s]
15573it [00:00, 25464.49it/s]
18382it [00:00, 26198.38it/s]
20886it [00:00, 21376.29it/s]
23088it [00:00, 21540.18it/s]
25803it [00:01, 22960.72it/s]
28468it [00:01, 23954.75it/s]
31131it [00:01, 24698.23it/s]
33850it [00:01, 25395.44it/s]
36545it [00:01, 25840.99it/s]
36850it [00:01, 24910.33it/s]

0it [00:00, ?it/s]
1655it [00:00, 12967.46it/s]
4257it [00:00, 15264.14it/s]
4940it [00:00, 19486.76it/s]

0it [00:00, ?it/s]
2579it [00:00, 25780.37it/s]
4107it [00:00, 21372.52it/s]
4940it [00:00, 19167.01it/s]
/data/kkzhang/miniconda3/envs/absa/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=1.0 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Loading data...
Vocab size: 4762
Time usage: 0:00:02
<bound method Module.parameters of Model(
  (embedding): Embedding(4762, 300)
  (lstm): LSTM(300, 256, batch_first=True, dropout=1.0, bidirectional=True)
  (maxpool): MaxPool1d(kernel_size=32, stride=32, padding=0, dilation=1, ceil_mode=False)
  (fc): Linear(in_features=812, out_features=3, bias=True)
)>
Epoch [1/10]
Iter:      0,  Train Loss:  0.94,  Train Acc: 74.22%,  Val Loss:  0.72,  Val Acc: 77.78%,  Time: 0:00:00 *
Iter:    100,  Train Loss:  0.61,  Train Acc: 79.69%,  Val Loss:  0.63,  Val Acc: 77.96%,  Time: 0:00:01 *
Iter:    200,  Train Loss:  0.66,  Train Acc: 76.56%,  Val Loss:  0.61,  Val Acc: 78.74%,  Time: 0:00:03 *
Epoch [2/10]
Iter:    300,  Train Loss:  0.69,  Train Acc: 75.00%,  Val Loss:   0.6,  Val Acc: 79.07%,  Time: 0:00:04 *
Iter:    400,  Train Loss:  0.52,  Train Acc: 84.38%,  Val Loss:   0.6,  Val Acc: 79.11%,  Time: 0:00:05 *
Iter:    500,  Train Loss:  0.61,  Train Acc: 78.12%,  Val Loss:  0.61,  Val Acc: 79.24%,  Time: 0:00:06 
Epoch [3/10]
Iter:    600,  Train Loss:  0.54,  Train Acc: 79.69%,  Val Loss:  0.61,  Val Acc: 79.21%,  Time: 0:00:07 
Iter:    700,  Train Loss:  0.44,  Train Acc: 87.50%,  Val Loss:   0.6,  Val Acc: 79.03%,  Time: 0:00:08 
Iter:    800,  Train Loss:  0.47,  Train Acc: 83.59%,  Val Loss:  0.61,  Val Acc: 79.03%,  Time: 0:00:09 
Epoch [4/10]
Iter:    900,  Train Loss:  0.33,  Train Acc: 88.28%,  Val Loss:  0.65,  Val Acc: 79.07%,  Time: 0:00:10 
Iter:   1000,  Train Loss:  0.51,  Train Acc: 78.12%,  Val Loss:  0.62,  Val Acc: 77.59%,  Time: 0:00:11 
Iter:   1100,  Train Loss:  0.38,  Train Acc: 84.38%,  Val Loss:  0.63,  Val Acc: 78.80%,  Time: 0:00:12 
Epoch [5/10]
Iter:   1200,  Train Loss:  0.52,  Train Acc: 78.12%,  Val Loss:  0.63,  Val Acc: 78.43%,  Time: 0:00:13 
Iter:   1300,  Train Loss:  0.35,  Train Acc: 89.84%,  Val Loss:  0.64,  Val Acc: 76.21%,  Time: 0:00:14 
Iter:   1400,  Train Loss:  0.38,  Train Acc: 83.59%,  Val Loss:  0.67,  Val Acc: 78.62%,  Time: 0:00:15 
No optimization for a long time, auto-stopping...
Test Loss:  0.59,  Test Acc: 79.75%

Time usage: 0:00:00
